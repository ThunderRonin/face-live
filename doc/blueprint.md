# Face Liveness Flutter Plugin – Technical Design & Implementation Guide

## Introduction

Face liveness detection is a process used to verify that a face presented to a camera is from a live person rather than a photo or video replay. This is crucial in eKYC and biometric authentication systems – for example, when verifying a selfie against an ID photo, the app must ensure the user is real and not just showing a static image[\[1\]](https://medium.com/kbtg-life/real-time-machine-learning-with-flutter-camera-bbcf1b5c3193?source=post_page-----6997edd28b29---------------------------------------#:~:text=My%20task%20was%20to%20implement,and%20respond%20back%20to%20Flutter). To achieve this, we will create a **Flutter plugin** named **face_liveness** that provides a high-performance native camera feed with real-time face detection. The plugin leverages Google’s ML Kit on-device face detection to track **head movements** as a liveness cue. By handling camera preview and ML processing natively (on iOS and Android) and only sending minimal results to Flutter, we avoid expensive image data transfers between Dart and native layers[\[2\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=In%20this%20version%2C%20I%20plan,performance%20until%20I%20implement%20it). This design maximizes speed and efficiency, enabling real-time performance even on moderate devices.

**Key Objectives:**  
\- **High Performance:** Use native camera APIs (AVFoundation on iOS, CameraX on Android) to stream frames directly to ML Kit, eliminating the need to shuffle image data through Flutter[\[2\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=In%20this%20version%2C%20I%20plan,performance%20until%20I%20implement%20it). Only liveness results (like progress and final outcome) are sent to Flutter, reducing overhead.  
\- **Face Movement Detection:** Configure ML Kit’s face detector to provide **Euler Y-axis rotation** (yaw) of the head. We will prompt or monitor the user turning their head left-right and use this motion as proof of liveness. The plugin will compute a **head-tracking progress (0–100%)** based on how much the user has moved their head relative to a target range.  
\- **Native Video Recording:** While tracking liveness, record a short video of the session (using native video capture) so that upon successful completion, the plugin can return a video file of the user’s liveness check.  
\- **Simple Flutter Integration:** Provide a Flutter widget (e.g. FaceLivenessView) that displays the native camera preview in the Flutter UI and offers callback properties for liveness progress updates and completion. This widget should be easy to integrate into any Flutter app, handling all platform communication internally.

By meeting these objectives, developers (or even an AI coding agent) can follow this design to implement the plugin with precision, resulting in a robust face liveness detection component.

## System Architecture Overview

**Plugin Structure:** The face_liveness plugin is a **platform plugin** with native iOS and Android code plus a Dart API. It uses Flutter’s method channels and platform views to bridge between Flutter and the device’s camera/ML capabilities[\[3\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=Because%20this%20plugin%20uses%20platform,your%20ML%20model%20and%2For%20app). Key components include:

- **Native Camera Preview** (Platform View): A native camera view (using **AVCaptureVideoPreviewLayer** on iOS and **CameraX PreviewView** on Android) is embedded in the Flutter widget tree. This provides a high-performance live camera feed (front-facing by default) as a UI element. The camera feed never passes through Dart; it’s rendered via the platform’s GPU for efficiency.
- **ML Kit Face Detection:** On each platform, Google’s ML Kit Face Detection is integrated to process camera frames in real-time. The ML Kit detectors run natively (Swift/Objective-C on iOS, Kotlin/Java on Android) for performance. As the camera captures frames, these frames are analyzed to detect faces and extract landmarks like Euler angles. **No image processing is done in Dart**; the plugin simply invokes ML Kit’s native API and receives results, behaving as a bridge[\[3\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=Because%20this%20plugin%20uses%20platform,your%20ML%20model%20and%2For%20app).
- **Liveness Logic (Head Movement):** The native code implements logic to track the user’s head movement using ML Kit’s output. Specifically, it uses **headEulerAngleY** (yaw – rotation about vertical axis) from the detected face to determine how far left/right the user has turned their head[\[4\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=final%20double%3F%20rotX%20%3D%20face,is%20tilted%20sideways%20rotZ%20degrees). The plugin maintains a progress metric (0–100%) representing the fraction of the required motion completed. For example, 0% when starting centered, and 100% when the user has turned their head to the target extents (say, sufficiently left and right). We may determine that “liveness confirmed” when the user has rotated their head across a certain angle range (e.g., ±30° yaw).
- **Video Recording:** In parallel with detection, the plugin records the camera feed to a video file. This ensures that upon successful liveness verification, we have a recorded proof of the session. The recording can start when the liveness check begins and stop when it ends. We will use platform-specific media recording facilities: on iOS, an **AVCaptureMovieFileOutput** or similar AVFoundation component to save the video, and on Android, the CameraX **VideoCapture** API or MediaRecorder to save an MP4 file.
- **Method Channels for Events:** A Flutter **MethodChannel** (or possibly EventChannel for continuous updates) is used to send data from native to Dart. Specifically, the plugin will send: (a) periodic **progress updates** (the head movement percentage), and (b) a **completion event** with the path/URI of the saved video file once liveness is verified. The Flutter side will receive these and invoke the appropriate callbacks. This follows the pattern of existing plugins – for example, an Android implementation might call methodChannel.invokeMethod("onProgress", progressValue) to update, and invokeMethod("onLivenessSuccess", videoPath) when done. Because we send only lightweight data (floats and strings), this is efficient. (In comparison, earlier naive approaches that transferred image bytes through the channel suffered major performance issues[\[5\]](https://medium.com/kbtg-life/real-time-machine-learning-with-flutter-camera-bbcf1b5c3193?source=post_page-----6997edd28b29---------------------------------------#:~:text=I%20took%20a%20photo%20by,test%20the%20face%20liveness%20feature).)

**Workflow Summary:**  
1\. **Camera Initialization:** When the Flutter widget is inserted in the UI, it creates the native camera view (via platform view). The plugin native code initializes the **front-facing** camera and starts the preview[\[6\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel). It also configures ML Kit’s face detector with appropriate options (enable head angle detection, possibly faster performance mode, etc.).  
2\. **Real-time Detection Loop:** As frames come in, the face detector runs on each (or throttled subset) frame in the native layer. If a face is found, we retrieve the Euler Y angle. The plugin updates the cumulative head movement progress – for instance, tracking the max left and right angles seen so far. We then send the updated **progress percentage** to Flutter via the method channel so the Flutter app can update a progress indicator UI if needed (e.g., a progress bar or text).  
3\. **Liveness Confirmation:** Once the head movement progress reaches 100% (meaning the user has turned their head sufficiently), the native code will consider the liveness check passed. At this point, it stops the camera and detection, finalizes the video recording, and saves the video file (e.g., to the app’s documents or cache directory). Then it signals Flutter (over the channel) with an **onCapture** event containing the video file path. If the user fails to complete the motion within a timeout or if no face is detected, the plugin can report a failure or timeout event accordingly (this behavior can be defined as needed).  
4\. **Resource Cleanup:** After completion (success or fail), the native code will release the camera, stop the capture session, and release the ML Kit detector to free resources. The Flutter widget can then be disposed or reset for another attempt as needed.

This architecture ensures that all heavy operations (camera streaming, ML inference, video encoding) happen in highly optimized native code, while Flutter handles only the UI and high-level logic. Similar approaches have been used in existing Flutter liveness or scanner plugins with great success, e.g. using CameraX with ML Kit for real-time processing[\[6\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel).

## Implementation Details

### 1\. Plugin Project Setup

Use the Flutter CLI to create a plugin project:  

flutter create --template=plugin --platforms=ios,android face_liveness

This generates a plugin structure with an ios/Classes folder for iOS code and an android/src/main for Android code, plus the Dart package scaffolding. We will implement platform-specific code in these native folders and expose a Dart API for the Flutter app. Key decisions at this stage: we’ll use **Swift** for iOS (to easily use AVFoundation and ML Kit) and **Kotlin** for Android. Ensure the pubspec includes ios and android under **plugin** platforms, and add dependencies for ML Kit: we can use the official **Google ML Kit Face Detection** libraries for each platform. The Flutter pub.dev package google_mlkit_face_detection can be a reference, but since we want the processing in native, we may directly integrate the ML Kit native SDKs (e.g., via CocoaPods for iOS and Gradle/Maven for Android). Alternatively, we could use the Dart ML Kit plugin by feeding it images, but that would negate some performance gains; thus native integration is preferred.

**iOS Dependencies:** In the iOS podspec or Swift Package, add dependencies for MLKit’s FaceDetection. Google’s MLKit for Vision (Face Detection) can be installed via CocoaPods by adding Firebase/MLVision or the standalone MLKit pods. We must also add the MLKit binary (xcframework) to the plugin. (The Medium reference suggests embedding an .xcframework and adjusting the .podspec[\[7\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=To%20embed%20the%20library%20into,Let%20me%20explain), but since we use CocoaPods, we can specify MLKit pods directly). The **minimum iOS deployment target must be 15.5** due to ML Kit’s requirements[\[8\]](https://pub.dev/packages/face_camera#:~:text=,iOS), and 32-bit architectures are not supported by MLKit[\[9\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=,More%20info%20here) (we’ll exclude armv7 in Xcode settings).

**Android Dependencies:** In the Android module’s build.gradle, include the ML Kit Vision dependency for face detection. For example, add implementation "com.google.mlkit:face-detection:latest-version" to the plugin’s Gradle config. We will use CameraX, so also include CameraX dependencies (androidx.camera:camera-core, camera-camera2, camera-lifecycle, camera-view etc., and camera-video if using VideoCapture). Ensure the **minSdkVersion is 21 or higher**[\[10\]](https://pub.dev/packages/face_camera#:~:text=,file) to support CameraX and ML Kit. Target and compile SDK should be updated to the latest (e.g., 33 or above as of this writing).

### 2\. Native Camera View (Platform View) Integration

To display the camera preview inside a Flutter widget, we register a **Platform View** factory in the plugin. This allows Flutter to embed a native UI component. In Dart, we’ll create a FaceLivenessView widget that uses AndroidView (for Android) and UiKitView (for iOS) to host the native camera view.

**Android Platform View:** Implement an AndroidView by creating a class (say FaceLivenessCameraView) that extends PlatformView and a FaceLivenessCameraViewFactory that extends PlatformViewFactory. When Flutter creates the widget, the factory’s create method will return an instance of a **PreviewView** (from CameraX) or a custom FrameLayout containing the camera preview. In FaceLivenessCameraView’s constructor, set up the CameraX preview. Use CameraX’s Jetpack libraries: obtain a ProcessCameraProvider and bind a **Preview** use case and an **ImageAnalysis** use case (for ML Kit) and a **VideoCapture** use case (for recording) to a lifecycle. Since we are in a plugin, we might need to use a **Flutter lifecycle** or the activity’s lifecycle – the plugin can get an Activity instance via the Flutter plugin binding. We will bind the **front camera** (CameraSelector.DEFAULT_FRONT_CAMERA). The Preview use case is attached to a PreviewView (which is a View that displays camera feed). The ImageAnalysis use case can receive frames and pass them to ML Kit’s face detector. We should configure ImageAnalysis at a suitable resolution (maybe 720p for balance of quality/performance) and set its backpressure strategy (drop frames if needed) to keep analysis real-time. Each frame will come as an ImageProxy, which we convert to the format needed by ML Kit (typically ML Kit can take a MediaImage plus rotation degrees directly). We run the face detector on a background thread (CameraX allows specifying an executor for ImageAnalysis). When faces are detected, retrieve the headEulerAngleY from the **Face** result (ML Kit’s Face class)[\[4\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=final%20double%3F%20rotX%20%3D%20face,is%20tilted%20sideways%20rotZ%20degrees). Use this to update progress (more on logic in section 3 below).

**iOS Platform View:** On iOS, implement a FlutterPlatformViewFactory and FlutterPlatformView (since we’re likely using Swift, we can use NSObject implementing FlutterPlatformView). This will create a **UIView** that contains the camera preview. We use AVFoundation: create an **AVCaptureSession** with the front camera (AVCaptureDevice.default(.builtInWideAngleCamera, .front, ...)). Add an **AVCaptureVideoPreviewLayer** to show the camera feed on our view’s layer. For analysis, we have two options: use **AVCaptureVideoDataOutput** to get frame buffers and run ML Kit’s face detection on those buffers, or use **Vision** framework. Since we want ML Kit, we will likely use ML Kit’s iOS API: e.g., MLKVisionImage created from CMSampleBuffer, and the MLKit face detector (MLKFaceDetector) to process it. We configure the AVCaptureSession to a reasonable resolution and set the **videoDataOutput** to deliver frames in a suitable format (maybe kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange for efficiency). The frames are handled in a delegate (captureOutput(didOutput:)). In that callback, we pass the CMSampleBuffer to ML Kit’s detector. If a face is detected, get the Euler Y angle from the MLKit Face object (ML Kit’s iOS API has similar output). Update progress accordingly and send to Dart. The camera preview layer will show the live video in the UI. We also add an **AVCaptureMovieFileOutput** to the session to record video: when liveness check starts, instruct it to start recording to a temporary file (specify output URL, call startRecording). Stop the recording when done.

**Threading & Performance:** Both platforms must handle detection off the main UI thread to keep UI smooth. CameraX’s ImageAnalysis with a dedicated executor takes care of background threading. On iOS, AVCaptureVideoDataOutput can deliver frames on a dispatch queue. We will perform ML Kit detection on that background queue. ML Kit’s processing is efficient (usually can do several FPS depending on device). If needed, we might throttle detection (e.g., only process every Nth frame or every X milliseconds) to balance performance vs. responsiveness. However, given native execution and possibly using hardware acceleration, we aim for real-time (around 15-30 FPS analysis if possible).

### 3\. Face Liveness Detection Logic (Head Movement Tracking)

The core liveness check is based on **head rotation**. We configure ML Kit’s face detector with appropriate options: it should have **pose detection enabled** (so it returns Euler angles). In ML Kit’s Dart API, Euler Y is described as “Head is rotated to the right rotY degrees”[\[4\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=final%20double%3F%20rotX%20%3D%20face,is%20tilted%20sideways%20rotZ%20degrees) – meaning if the person turns their head to their right (looking left from camera perspective), that angle is positive or negative depending on convention. We will clarify MLKit’s convention (commonly, a face turned to the **left** (person's left) yields a negative yaw, to the right yields positive, but we can empirically determine during testing).

**Progress Calculation:** We define a target yaw range that the user’s head must cover, for example from -30° to +30° (total range 60°). Initially, when the face is centered, yaw ~0. As the user turns left or right, we track the **minimum and maximum** yaw observed. We can compute progress as the span of angles covered relative to the target span. For instance, if so far the user has turned from -10° (left) to +15° (right), that’s a 25° span out of the required 60°, ~41% progress. When they turn further to cover the full 60°, progress reaches 100%. We will continuously update this as:  

We ensure a face is detected throughout; if the face goes out of frame, we might pause progress or reset if too prolonged. Only when a single session’s continuous detection achieves full range do we declare success.

**Calibration:** Because every person’s range of comfortable motion differs, we might choose a slightly smaller required range to accommodate, or instruct the user via UI (e.g., “Turn your head left and right”). Optionally, we could enhance this by also requiring a **nod (Euler X)** or random direction prompt, but the requirement specifically focuses on Y-axis movement.

**ML Kit Configuration:** Use **FaceDetectorOptions** to set appropriate settings: high accuracy vs fast mode – since we just need pose, a _fast mode_ might suffice unless accuracy of angle is critical. We should enable **face tracking** (so that ML Kit assigns an ID to a face) to ensure we are dealing with the same face throughout the session[\[11\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=%2F%2F%20If%20face%20tracking%20was,trackingId%3B). This can help maintain state if multiple faces (though in liveness, we expect only one face in view). We do not necessarily need landmarks or classification (like eyes open or smiling) for the head movement method, so those can be off to improve speed.

**Handling Results:** Each time we get a new face detection result with an updated angle, we compute the new progress percentage. Then:  
\- Call a MethodChannel method (say "onProgress") with the progress value (as an int or double). On the Dart side, this triggers the user-provided onProgress callback with that value. This allows the Flutter UI to update a progress bar or textual indicator in real time (e.g., “Liveness progress: 45%”).  
\- If progress reaches 100%, we trigger completion: stop further detection and proceed to finalize the output.

**Edge cases:** If no face is detected in a frame, we might ignore that frame (keep waiting) and possibly notify Flutter if needed (the UI could show “face not in frame” – we could provide another callback or include in progress as -1 or so, but simplicity: maybe just handle in UI by no updates). If multiple faces (unlikely in front camera scenario for liveness), we can choose the most prominent (ML Kit returns a list of faces; we take the one with largest bounding box or just first since presumably one face). If the user completes the motion too quickly or too slowly, we ensure the logic still catches the full range. We may impose a minimum time to ensure they did it deliberately (but not required). If after a certain timeout (e.g., 10 seconds) progress is still incomplete, we could abort and notify failure.

### 4\. Video Recording Implementation

Simultaneously, the plugin records the session video. This is straightforward: we start recording when liveness detection begins (once the camera is initialized and user’s face is found, or immediately when the widget is loaded, depending on design). On **Android**, using CameraX, we can bind a **VideoCapture** use case to the camera. For example, with cameraProvider.bindToLifecycle(lifecycleOwner, cameraSelector, preview, analysis, videoCapture). Then call videoCapture.startRecording(outputOptions, executor, callback). The outputOptions specify a file path (e.g., in cache directory with a timestamped filename.mp4). When we stop, we call videoCapture.stopRecording(). The CameraX VideoCapture will handle encoding. Alternatively, one could manually use MediaRecorder, but CameraX simplifies it. On **iOS**, we add an AVCaptureMovieFileOutput to the AVCaptureSession. We call startRecording(to: fileURL, recordingDelegate: self) to begin. We implement the delegate to handle events (e.g., fileOutput(\_:, didFinishRecordingTo: ...)). The output file URL can point to app’s temporary directory. We must ensure to stop recording when liveness is achieved or if the check is canceled.

After recording stops, we will have a video file saved. We then communicate its path to Flutter in the completion callback (MethodChannel). For instance, on iOS after didFinishRecordingTo, we call the Dart callback with the file path; on Android, in the VideoCapture callback or after calling stop, we do the same via method channel. The Flutter side’s onCapture(File videoFile) (or perhaps onCapture(String path)) is then invoked with this file, allowing the app to play it back or upload it.

**File management:** The plugin should document where the video is saved (cache or temporary folder) and whose responsibility it is to delete or move it. It’s reasonable to save in app cache and let the app decide (maybe the example app will just print the path and the developer can use it).

### 5\. Flutter API (Dart Side)

On the Dart side, we provide a widget and controller interface to use this plugin easily. For example:

class FaceLivenessView extends StatefulWidget {  
final void Function(double progress)? onProgress;  
final void Function(File video)? onCapture;  
// (We could also add config parameters like targetYaw or timeout if needed)  
<br/>FaceLivenessView({this.onProgress, this.onCapture, Key? key}) : super(key: key);  
<br/>@override  
\_FaceLivenessViewState createState() => \_FaceLivenessViewState();  
}

The state will create the platform view. We might not even need a separate controller class unless we want to expose controls (like switching camera, which in liveness we likely keep front only). The simplest approach: in build, return:

Widget build(BuildContext context) {  
return Platform.isIOS  
? UiKitView(viewType: 'face_liveness_camera', onPlatformViewCreated: \_onViewCreated)  
: AndroidView(viewType: 'face_liveness_camera', onPlatformViewCreated: \_onViewCreated);  
}

The viewType string is the unique identifier we register in native code (e.g., "face_liveness_camera"). The \_onViewCreated can establish a MethodChannel connection to listen for events if not already set up via binary messenger. However, an alternative is to have the native side of the plugin set up a MethodChannel in the plugin’s registrar and send events directly (which might be simpler: the MethodChannel can be a singleton per plugin instance). In that case, we can set up a MethodChannel('face_liveness') in Dart and set a method call handler to handle "onProgress" and "onLivenessSuccess" calls. This handler would simply forward to widget callbacks: e.g., if method == "onProgress", call widget.onProgress(percent); if "onLivenessSuccess", call widget.onCapture(File(path)). The plugin’s Dart side can manage this through a singleton or by assigning the events to the correct widget. Since potentially multiple FaceLivenessView widgets might be used (though unlikely at same time), a more robust design is to include an id in the MethodChannel calls or create a distinct channel for each instance (perhaps passing a viewId). Simpler: we can use the view’s platformChannel by viewId (PlatformView has a channel for communication). For this document, we’ll assume a single instance at a time and use a fixed channel name for clarity.

**Flutter Callbacks:** The widget’s onProgress and onCapture are provided by the app. For example, a developer can do:

FaceLivenessView(  
onProgress: (percent) => print("Liveness progress: $percent%"),  
onCapture: (file) {  
print("Liveness success, video recorded at: ${file.path}");  
// e.g., play the video or upload it  
},  
)

This makes integration simple and aligns with the requirements (the example will print to console as specified).

We also expose any needed controls. In our case, we might not need interactive controls (no need for user to manually capture photo since it’s automatic). If desired, a FaceLivenessController could be made to allow programmatic control (start/stop), but the design can be simplified: the liveness check starts as soon as the widget appears (camera opens automatically). For completeness, a controller can be provided to start/stop or retry the process if needed. This is similar to how the face_camera plugin exposes a controller with options[\[12\]](https://pub.dev/packages/face_camera#:~:text=%40override%20void%20initState%28%29%20,front%2C%20onCapture%3A%20%28File%3F%20image%29)[\[13\]](https://pub.dev/packages/face_camera#:~:text=%40override%20Widget%20build%28BuildContext%20context%29%20,), but we can start automatically.

### 6\. Permissions and App Integration

Because we’re dealing with camera (and microphone for video), the app must have appropriate permissions:

- **iOS:** Add usage descriptions to **Info.plist**. Specifically, add keys **NSCameraUsageDescription** and **NSMicrophoneUsageDescription** with strings explaining why the app needs them (e.g., “Camera access is required for face liveness detection” and “Microphone access is required for recording video audio”)[\[14\]](https://pub.dev/packages/face_camera#:~:text=Add%20two%20rows%20to%20the,plist). The plugin itself cannot add these; it's the app’s responsibility, but we will document this clearly. The iOS implementation will automatically prompt the user for permission at runtime when starting the capture session. (Alternatively, we could trigger an AVCaptureDevice requestAccess if we want to handle permission flow more gracefully).
- **Android:** Add the camera permission in **AndroidManifest.xml**:  

- &lt;uses-permission android:name="android.permission.CAMERA" /&gt;
- If recording audio, also include:  

- &lt;uses-permission android:name="android.permission.RECORD_AUDIO" /&gt;
- At runtime, starting from Android 6.0, the app must request these permissions from the user. The plugin can either rely on the app to handle it or use a permissions plugin. For simplicity, we assume the app will request permission (our example can use the permission_handler package to do so)[\[15\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=You%20must%20add%20camera%20permissions,AndroidManifest.xml). We will note this in documentation: the user must grant camera (and mic) permission before the liveness widget is used.

Once permissions are granted, using the FaceLivenessView in the app should just work. The widget will initialize the camera stream and begin detection. The app should ensure to place the widget in a context that's visible and has adequate size (full-screen or a large container, since the user needs to align their face). We might overlay some guidance UI on top of the camera feed – for example, a text telling the user to **“Turn your head left and right until the scan completes”**. The plugin could optionally provide a basic overlay or the app can do it (since Flutter can stack widgets over the camera view).

## Testing and Example Usage

We will provide an **example Flutter app** in the plugin’s example/ directory to demonstrate usage and allow testing. This app will include a single screen with the FaceLivenessView. It will log the progress and result: for instance, using the callbacks to print updates. When running the example, one can see in real-time the progress percentage rising as they move their head. Once it hits 100%, the onCapture callback will fire and print the video file path (and perhaps play the video or show a success message).

**Manual Testing Scenarios:**  
\- Test on both Android and iOS physical devices (since camera and ML performance can’t be fully validated on simulators – note: iOS simulators won’t support the camera, so iOS testing must be on device). - Ensure that when the face moves in the required manner, progress smoothly goes to 100 and triggers success. - Test boundary cases: if the user only turns one direction and back to center (not reaching opposite side), ensure progress calculation works (it should track the extremities correctly). - Test what happens if the user moves too quickly or turns out of frame: the progress might pause; ensure no crashes. - Verify the video file is saved and contains the expected footage (the user’s face turning). Check that the file path is accessible and the video playable. - Verify that if the user stops or the widget is removed mid-process (e.g., navigating away), the camera is properly released (to avoid locking the camera hardware). - Check CPU/memory usage to confirm the performance improvement. We expect, based on similar implementations, significantly lower latency and CPU than a pure Dart image-stream approach[\[16\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=This%20led%20me%20to%20explore,in%20the%20name%20of%20%E2%80%98mobile_scanner%E2%80%99)[\[17\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=The%20results%20indicate%20a%20significant,being%20unusable%20to%20finally%20usable). The native approach should maintain a high frame rate and low lag, as observed in the reference project (where a move from Flutter-side ML to native MLKit made liveness detection **“finally usable” even on low-end devices**[**\[18\]**](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=FaceLiveness%20with%20Native%20Camera%2C%20iPhone,14%20Pro%20Max)).

**Accuracy Considerations:**  
The accuracy of this liveness check depends on ML Kit’s face detection reliability and the chosen movement threshold. ML Kit’s face detection is quite robust for frontal faces and provides Euler angles with reasonable accuracy. However, this simple head-turn test is a basic liveness check – it will prevent use of a single photo, but might not stop more sophisticated spoofs (like an animated deepfake or a video of a person). In practice, blinking or smile detection are also often used[\[19\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel), or even specialized liveness models (e.g., depth via infrared, reflections, etc.). Our design could be extended: for example, enabling **eye blink detection** (ML Kit provides leftEyeOpenProbability) or smile (smilingProbability)[\[20\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=%2F%2F%20If%20classification%20was%20enabled,smilingProbability%3B) and requiring an eye blink as an additional check. For now, our focus is head movement. The app can also randomize the required movement (“turn left then right”) to make it harder to use a prerecorded video. In terms of angle accuracy, a tolerance can be applied – we don’t need exact degrees, just enough range to indicate 3D movement of a real face. ML Kit’s real-time output should suffice. We will test that the chosen angle range and progress logic result in a smooth user experience (not too easy but not too hard to achieve 100%).

## Conclusion

In this design, we outlined a step-by-step approach to implement the face_liveness Flutter plugin with a native-first methodology. By using the device’s native camera frameworks and Google ML Kit, we ensure that face liveness detection runs efficiently and accurately, sending only minimal data back to Flutter[\[2\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=In%20this%20version%2C%20I%20plan,performance%20until%20I%20implement%20it)[\[3\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=Because%20this%20plugin%20uses%20platform,your%20ML%20model%20and%2For%20app). The plugin will provide a friendly Flutter API (FaceLivenessView widget) that can be dropped into any app. Internally, it opens the front camera[\[6\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel), analyzes the user’s face in real time, tracks head movement via Euler angles[\[4\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=final%20double%3F%20rotX%20%3D%20face,is%20tilted%20sideways%20rotZ%20degrees), and when liveness criteria are met, returns a recorded video of the session. We have also detailed the necessary setup (permissions, dependencies) and the logic to calculate progress and determine success.

By following this technical design document, a developer or AI code assistant can implement the plugin systematically. Each component – iOS, Android, and Dart – has a clear role and interface. The end result will be a high-performance face liveness detection plugin that enhances security for Flutter applications, with an architecture similar to proven solutions in the community (e.g., using CameraX + MLKit as in barcode scanners and liveness plugins[\[19\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel)). This design is both **precise** and **comprehensive**, ensuring that the implementation can be carried out with minimal guesswork and yielding a reliable feature for end-users.

**Testing the Outcome:** To validate, run the included example app on a device. You should see the camera preview and logs of progress. Move your head left and right; once the progress hits 100%, you’ll get a console message with the video file path[\[21\]\[22\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel). This confirms the liveness check passed and the video was captured, meeting all the requirements of the project. Enjoy coding, and happy verifying live faces!

**Sources:** The design principles and implementation strategies referenced here draw upon best practices and examples from the Flutter community, including the **face_camera** plugin (real-time face capture)[\[23\]](https://pub.dev/packages/face_camera#:~:text=,iOS), **google_mlkit_face_detection** (ML Kit integration in Flutter)[\[3\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=Because%20this%20plugin%20uses%20platform,your%20ML%20model%20and%2For%20app)[\[4\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=final%20double%3F%20rotX%20%3D%20face,is%20tilted%20sideways%20rotZ%20degrees), and insightful case studies of Flutter liveness detection in production[\[1\]](https://medium.com/kbtg-life/real-time-machine-learning-with-flutter-camera-bbcf1b5c3193?source=post_page-----6997edd28b29---------------------------------------#:~:text=My%20task%20was%20to%20implement,and%20respond%20back%20to%20Flutter)[\[2\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=In%20this%20version%2C%20I%20plan,performance%20until%20I%20implement%20it)[\[19\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel). These resources provide a solid foundation that we have built upon to create an advanced yet practical face liveness plugin.

[\[1\]](https://medium.com/kbtg-life/real-time-machine-learning-with-flutter-camera-bbcf1b5c3193?source=post_page-----6997edd28b29---------------------------------------#:~:text=My%20task%20was%20to%20implement,and%20respond%20back%20to%20Flutter) [\[5\]](https://medium.com/kbtg-life/real-time-machine-learning-with-flutter-camera-bbcf1b5c3193?source=post_page-----6997edd28b29---------------------------------------#:~:text=I%20took%20a%20photo%20by,test%20the%20face%20liveness%20feature) Real-time Machine Learning with Flutter Camera | KBTG Life

<https://medium.com/kbtg-life/real-time-machine-learning-with-flutter-camera-bbcf1b5c3193?source=post_page-----6997edd28b29--------------------------------------->

[\[2\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=In%20this%20version%2C%20I%20plan,performance%20until%20I%20implement%20it) [\[7\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=To%20embed%20the%20library%20into,Let%20me%20explain) [\[16\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=This%20led%20me%20to%20explore,in%20the%20name%20of%20%E2%80%98mobile_scanner%E2%80%99) [\[17\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=The%20results%20indicate%20a%20significant,being%20unusable%20to%20finally%20usable) [\[18\]](https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29#:~:text=FaceLiveness%20with%20Native%20Camera%2C%20iPhone,14%20Pro%20Max) Implementing Face Liveness Detection in Flutter | KBTG Life

<https://medium.com/kbtg-life/implementing-face-liveness-detection-in-flutter-with-high-performance-6997edd28b29>

[\[3\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=Because%20this%20plugin%20uses%20platform,your%20ML%20model%20and%2For%20app) [\[4\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=final%20double%3F%20rotX%20%3D%20face,is%20tilted%20sideways%20rotZ%20degrees) [\[9\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=,More%20info%20here) [\[11\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=%2F%2F%20If%20face%20tracking%20was,trackingId%3B) [\[20\]](https://pub.dev/packages/google_mlkit_face_detection#:~:text=%2F%2F%20If%20classification%20was%20enabled,smilingProbability%3B) google_mlkit_face_detection | Flutter package

<https://pub.dev/packages/google_mlkit_face_detection>

[\[6\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel) [\[15\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=You%20must%20add%20camera%20permissions,AndroidManifest.xml) [\[19\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel) [\[21\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel) [\[22\]](https://pub.dev/packages/flutter_liveness_plugin#:~:text=,MethodChannel) flutter_liveness_plugin | Flutter package

<https://pub.dev/packages/flutter_liveness_plugin>

[\[8\]](https://pub.dev/packages/face_camera#:~:text=,iOS) [\[10\]](https://pub.dev/packages/face_camera#:~:text=,file) [\[12\]](https://pub.dev/packages/face_camera#:~:text=%40override%20void%20initState%28%29%20,front%2C%20onCapture%3A%20%28File%3F%20image%29) [\[13\]](https://pub.dev/packages/face_camera#:~:text=%40override%20Widget%20build%28BuildContext%20context%29%20,) [\[14\]](https://pub.dev/packages/face_camera#:~:text=Add%20two%20rows%20to%20the,plist) [\[23\]](https://pub.dev/packages/face_camera#:~:text=,iOS) face_camera | Flutter package

<https://pub.dev/packages/face_camera>